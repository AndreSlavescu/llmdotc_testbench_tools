{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyM9676xFhU0h6HBmeCvhr5N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AndreSlavescu/llmdotc_testbench_tools/blob/main/llmc_testbench.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile common.h\n",
        "#include <stdlib.h>\n",
        "#include <stdio.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include <cublas_v2.h>\n",
        "#include <cublasLt.h>\n",
        "#include <float.h>\n",
        "\n",
        "template<class T>\n",
        "__host__ __device__ T ceil_div(T dividend, T divisor) {\n",
        "    return (dividend + divisor-1) / divisor;\n",
        "}\n",
        "\n",
        "__device__ float warpReduceSum(float val) {\n",
        "    for (int offset = 16; offset > 0; offset /= 2) {\n",
        "        val += __shfl_xor_sync(0xFFFFFFFF, val, offset);\n",
        "    }\n",
        "    return val;\n",
        "}\n",
        "\n",
        "\n",
        "// CUDA error checking\n",
        "void cudaCheck(cudaError_t error, const char *file, int line) {\n",
        "  if (error != cudaSuccess) {\n",
        "    printf(\"[CUDA ERROR] at file %s:%d:\\n%s\\n\", file, line, cudaGetErrorString(error));\n",
        "    exit(EXIT_FAILURE);\n",
        "  }\n",
        "}\n",
        "#define cudaCheck(err) (cudaCheck(err, __FILE__, __LINE__))\n",
        "\n",
        "float* make_random_float(size_t N) {\n",
        "    float* arr = (float*)malloc(N * sizeof(float));\n",
        "    for (size_t i = 0; i < N; i++) {\n",
        "        arr[i] = ((float)rand() / RAND_MAX) * 2.0 - 1.0; // range -1..1\n",
        "    }\n",
        "    return arr;\n",
        "}\n",
        "\n",
        "template<class D, class T>\n",
        "void validate_result(D* device_result, const T* cpu_reference, const char* name, std::size_t num_elements, T tolerance=1e-4) {\n",
        "    D* out_gpu = (D*)malloc(num_elements * sizeof(D));\n",
        "    cudaCheck(cudaMemcpy(out_gpu, device_result, num_elements * sizeof(D), cudaMemcpyDeviceToHost));\n",
        "    int nfaults = 0;\n",
        "#ifndef ENABLE_BF16\n",
        "    float epsilon = FLT_EPSILON;\n",
        "#else\n",
        "    float epsilon = 0.079;\n",
        "#endif\n",
        "    for (int i = 0; i < num_elements; i++) {\n",
        "        // Skip masked elements\n",
        "        if(!isfinite(cpu_reference[i]))\n",
        "            continue;\n",
        "\n",
        "        // print the first few comparisons\n",
        "        if (i < 5) {\n",
        "            printf(\"%f %f\\n\", cpu_reference[i], (T)out_gpu[i]);\n",
        "        }\n",
        "        // effective tolerance is based on expected rounding error (epsilon),\n",
        "        // plus any specified additional tolerance\n",
        "        float t_eff = tolerance + fabs(cpu_reference[i]) * epsilon;\n",
        "        // ensure correctness for all elements.\n",
        "        if (fabs(cpu_reference[i] - (T)out_gpu[i]) > t_eff) {\n",
        "            printf(\"Mismatch of %s at %d: CPU_ref: %f vs GPU: %f\\n\", name, i, cpu_reference[i], (T)out_gpu[i]);\n",
        "            nfaults ++;\n",
        "            if (nfaults >= 10) {\n",
        "                free(out_gpu);\n",
        "                exit(EXIT_FAILURE);\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    if (nfaults > 0) {\n",
        "        free(out_gpu);\n",
        "        exit(EXIT_FAILURE);\n",
        "    }\n",
        "\n",
        "    free(out_gpu);\n",
        "}\n",
        "\n",
        "template<class Kernel, class... KernelArgs>\n",
        "float benchmark_kernel(int repeats, Kernel kernel, KernelArgs&&... kernel_args) {\n",
        "    cudaEvent_t start, stop;\n",
        "    // prepare buffer to scrub L2 cache between benchmarks\n",
        "    // just memset a large dummy array, recommended by\n",
        "    // https://stackoverflow.com/questions/31429377/how-can-i-clear-flush-the-l2-cache-and-the-tlb-of-a-gpu\n",
        "    // and apparently used in nvbench.\n",
        "    int deviceIdx = 0;\n",
        "    cudaCheck(cudaSetDevice(deviceIdx));\n",
        "    cudaDeviceProp deviceProp;\n",
        "    cudaCheck(cudaGetDeviceProperties(&deviceProp, deviceIdx));\n",
        "    void* flush_buffer;\n",
        "    cudaCheck(cudaMalloc(&flush_buffer, deviceProp.l2CacheSize));\n",
        "\n",
        "    cudaCheck(cudaEventCreate(&start));\n",
        "    cudaCheck(cudaEventCreate(&stop));\n",
        "    float elapsed_time = 0.f;\n",
        "    for (int i = 0; i < repeats; i++) {\n",
        "        // clear L2\n",
        "        cudaCheck(cudaMemset(flush_buffer, 0, deviceProp.l2CacheSize));\n",
        "        // now we can start recording the timing of the kernel\n",
        "        cudaCheck(cudaEventRecord(start, nullptr));\n",
        "        kernel(std::forward<KernelArgs>(kernel_args)...);\n",
        "        cudaCheck(cudaEventRecord(stop, nullptr));\n",
        "        cudaCheck(cudaEventSynchronize(start));\n",
        "        cudaCheck(cudaEventSynchronize(stop));\n",
        "        float single_call;\n",
        "        cudaCheck(cudaEventElapsedTime(&single_call, start, stop));\n",
        "        elapsed_time += single_call;\n",
        "    }\n",
        "\n",
        "    cudaCheck(cudaFree(flush_buffer));\n",
        "\n",
        "    return elapsed_time / repeats;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AvPJ_FD4smIh",
        "outputId": "223e9990-aed0-406f-fcb4-623e342c8d78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting common.h\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-s-c2NsOnTf",
        "outputId": "a8c81232-6d4d-4ec0-ba3d-4b0acdc3e558"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting rmsnorm_forward.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile rmsnorm_forward.cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include <assert.h>\n",
        "#include <cooperative_groups.h>\n",
        "#include <cooperative_groups/reduce.h>\n",
        "#include \"common.h\"\n",
        "\n",
        "// Root Mean Square Layernorm Forward Pass\n",
        "void rmsnorm_forward_cpu(\n",
        "    float *out,\n",
        "    float *rms,\n",
        "    const float *inp,\n",
        "    const float *weight,\n",
        "    const float *bias,\n",
        "    int B,\n",
        "    int T,\n",
        "    int C\n",
        ") {\n",
        "    const float eps = 1e-6f;\n",
        "\n",
        "    for (int b = 0; b < B; b++) {\n",
        "        for (int t = 0; t < T; t++) {\n",
        "            // seek to the input position inp[b,t,:]\n",
        "            const float* x = inp + b * T * C + t * C;\n",
        "            // compute RMS\n",
        "            float sum_of_squares = 0.0f;\n",
        "            for (int i = 0; i < C; i++) {\n",
        "                sum_of_squares += x[i] * x[i];\n",
        "            }\n",
        "            float rms_val = rsqrtf(sum_of_squares / C + eps);\n",
        "            // seek to the output position in out[b,t,:]\n",
        "            float* out_bt = out + b * T * C + t * C;\n",
        "            for (int i = 0; i < C; i++) {\n",
        "                float n = x[i] * rms_val; // normalized output\n",
        "                float o = n * weight[i] + bias[i]; // scale and shift it\n",
        "                out_bt[i] = o; // write\n",
        "            }\n",
        "            // cache the rms for the backward pass later\n",
        "            rms[b * T + t] = rms_val;\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "// ----------------------------------------------------------------------------\n",
        "// GPU kernels\n",
        "\n",
        "__global__ void rmsnorm_forward_kernel1(\n",
        "    float* out,\n",
        "    float* rms,\n",
        "    const float* inp,\n",
        "    const float* weight,\n",
        "    const float* bias,\n",
        "    int N,\n",
        "    int C\n",
        ") {\n",
        "    const float eps = 1e-6f;\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (idx < N) {\n",
        "        // Seek to the input position inp[idx,:]\n",
        "        const float* x = inp + idx * C;\n",
        "\n",
        "        // Calculate the sum of squares\n",
        "        float sum_of_squares = 0.0f;\n",
        "\n",
        "        #pragma unroll\n",
        "        for (int i = 0; i < C; i++) {\n",
        "            sum_of_squares += x[i] * x[i];\n",
        "        }\n",
        "\n",
        "        // Compute RMS value\n",
        "        sum_of_squares = sum_of_squares / C;\n",
        "        float rms_val = rsqrtf(sum_of_squares + eps);\n",
        "\n",
        "        // Seek to the output position in out[idx,:]\n",
        "        float* out_idx = out + idx * C;\n",
        "\n",
        "        #pragma unroll\n",
        "        for (int i = 0; i < C; i++) {\n",
        "            float n = x[i] * rms_val; // Normalized output\n",
        "            float o = n * weight[i] + bias[i]; // Scale and shift it\n",
        "            out_idx[i] = o; // Write\n",
        "        }\n",
        "\n",
        "        // Cache the RMS for the backward pass later\n",
        "        rms[idx] = rms_val;\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void rms_val_kernel(\n",
        "    float* rms,\n",
        "    const float* inp,\n",
        "    int N,\n",
        "    int C,\n",
        "    int block_size\n",
        ") {\n",
        "    extern __shared__ float shared[];\n",
        "    int idx = blockIdx.x; // range [0, B*T)\n",
        "    int tid = threadIdx.x; // range [0, blocksize]\n",
        "    const float *x = inp + idx * C;\n",
        "\n",
        "    const float eps = 1e-6f;\n",
        "    float sum_of_squares = 0.0f;\n",
        "\n",
        "    #pragma unroll\n",
        "    for (int i = tid; i < C; i += block_size) {\n",
        "        sum_of_squares += x[i] * x[i];\n",
        "    }\n",
        "    shared[tid] = sum_of_squares;\n",
        "    __syncthreads();\n",
        "\n",
        "    #pragma unroll\n",
        "    for (int stride = block_size >> 1; stride > 0; stride >>= 1) {\n",
        "        __syncthreads();\n",
        "        if (tid < stride) {\n",
        "            shared[tid] += shared[tid + stride];\n",
        "        }\n",
        "    }\n",
        "\n",
        "    if (tid == 0) {\n",
        "        rms[idx] = rsqrt(shared[0] / C + eps); // write back accumulated value in thread 0\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void rmsnorm_forward_kernel2(\n",
        "    float* out,\n",
        "    float* rms,\n",
        "    const float* inp,\n",
        "    const float* weight,\n",
        "    const float* bias,\n",
        "    int B,\n",
        "    int T,\n",
        "    int C\n",
        ") {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int bt = idx / C;\n",
        "    int c = idx % C;\n",
        "\n",
        "    float rms_val = rms[bt];\n",
        "    float xi = inp[idx];\n",
        "    float n = xi * rms_val;\n",
        "    float o = n * weight[c] + bias[c];\n",
        "\n",
        "    out[idx] = o;\n",
        "}\n",
        "\n",
        "__global__ void rmsnorm_forward_kernel3(\n",
        "    float* __restrict__ out,\n",
        "    float* __restrict__ rms,\n",
        "    const float* __restrict__ inp,\n",
        "    const float* __restrict__ weight,\n",
        "    const float* __restrict__ bias,\n",
        "    int B,\n",
        "    int T,\n",
        "    int C\n",
        ") {\n",
        "    namespace cg = cooperative_groups;\n",
        "    constexpr unsigned WARP_SIZE = 32;\n",
        "\n",
        "    int num_warps = blockDim.x / WARP_SIZE;\n",
        "    int lane_id = threadIdx.x % WARP_SIZE;\n",
        "    int warp_id = threadIdx.x / WARP_SIZE;\n",
        "    int idx = blockIdx.x;\n",
        "\n",
        "    __shared__ float shared[WARP_SIZE];\n",
        "    const float *x = inp + idx * C;\n",
        "\n",
        "    const float eps = 1e-6f;\n",
        "    float thread_sum_of_squares = 0.0f;\n",
        "\n",
        "    #pragma unroll\n",
        "    for (int i = threadIdx.x; i < C; i += blockDim.x) {\n",
        "        float xi = x[i];\n",
        "        thread_sum_of_squares += xi * xi;\n",
        "    }\n",
        "\n",
        "    cg::thread_block block = cg::this_thread_block();\n",
        "    cg::thread_block_tile<WARP_SIZE> warp = cg::tiled_partition<WARP_SIZE>(block);\n",
        "\n",
        "    float warp_sum_of_squares = cg::reduce(warp, thread_sum_of_squares, cg::plus<float>{}); // sum(x * x)\n",
        "    if (lane_id == 0) {\n",
        "        shared[warp_id] = warp_sum_of_squares;\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    warp_sum_of_squares = (lane_id < num_warps) ? shared[lane_id] : 0.0f;\n",
        "    float block_sum_of_squares = cg::reduce(warp, warp_sum_of_squares, cg::plus<float>{}); // sum(x * x)\n",
        "\n",
        "    // compute rms\n",
        "    float rms_val = rsqrtf(block_sum_of_squares / C + eps);\n",
        "    if (threadIdx.x == 0 && rms != nullptr) {\n",
        "        __stcs(rms + idx, rms_val);\n",
        "    }\n",
        "\n",
        "    float *o = out + idx * C;\n",
        "\n",
        "    #pragma unroll\n",
        "    for (int i = threadIdx.x; i < C; i += blockDim.x) {\n",
        "        float n =  __ldcs(x+i) * rms_val;\n",
        "        __stcs(o+i, n * weight[i] + bias[i]);\n",
        "    }\n",
        "}\n",
        "\n",
        "// ----------------------------------------------------------------------------\n",
        "// kernel launcher\n",
        "\n",
        "void rmsnorm_forward1(\n",
        "    float* out,\n",
        "    float* rms,\n",
        "    const float* inp,\n",
        "    const float* weight,\n",
        "    const float* bias,\n",
        "    int B,\n",
        "    int T,\n",
        "    int C,\n",
        "    const int block_size\n",
        ") {\n",
        "    const int N = B * T;\n",
        "    const int grid_size = ceil_div(N, block_size);\n",
        "    rmsnorm_forward_kernel1<<<grid_size, block_size>>>(out, rms, inp, weight, bias, N, C);\n",
        "    cudaCheck(cudaGetLastError());\n",
        "}\n",
        "\n",
        "void rmsnorm_forward2(\n",
        "    float* out,\n",
        "    float* rms,\n",
        "    const float* inp,\n",
        "    const float* weight,\n",
        "    const float* bias,\n",
        "    int B,\n",
        "    int T,\n",
        "    int C,\n",
        "    const int block_size\n",
        ") {\n",
        "    int N = B * T;\n",
        "    // in rms, threads cooperate within blocks via reductions\n",
        "    rms_val_kernel<<<B * T, block_size, block_size * sizeof(float)>>>(rms, inp, N, C, block_size);\n",
        "    cudaCheck(cudaGetLastError());\n",
        "    const int grid_size = ceil_div(B * T * C, block_size);\n",
        "    rmsnorm_forward_kernel2<<<grid_size, block_size>>>(out, rms, inp, weight, bias, B, T, C);\n",
        "    cudaCheck(cudaGetLastError());\n",
        "}\n",
        "\n",
        "void rmsnorm_forward3(\n",
        "    float* out,\n",
        "    float* rms,\n",
        "    const float* inp,\n",
        "    const float* weight,\n",
        "    const float* bias,\n",
        "    int B,\n",
        "    int T,\n",
        "    int C,\n",
        "    const int block_size\n",
        ") {\n",
        "    assert(block_size % 32 == 0);\n",
        "    const int N = B * T;\n",
        "    const int grid_size = N;\n",
        "    rmsnorm_forward_kernel3<<<grid_size, block_size>>>(out, rms, inp, weight, bias, B, T, C);\n",
        "    cudaCheck(cudaGetLastError());\n",
        "}\n",
        "\n",
        "// kernel version dispatch\n",
        "void rmsnorm_forward(\n",
        "    int kernel_num,\n",
        "    float* out,\n",
        "    float* rms,\n",
        "    const float* inp,\n",
        "    const float* weight,\n",
        "    const float* bias,\n",
        "    int B,\n",
        "    int T,\n",
        "    int C,\n",
        "    const int block_size\n",
        ") {\n",
        "    switch (kernel_num) {\n",
        "        case 1:\n",
        "            rmsnorm_forward1(out, rms, inp, weight, bias, B, T, C, block_size);\n",
        "            break;\n",
        "        case 2:\n",
        "            rmsnorm_forward2(out, rms, inp, weight, bias, B, T, C, block_size);\n",
        "            break;\n",
        "        case 3:\n",
        "            rmsnorm_forward3(out, rms, inp, weight, bias, B, T, C, block_size);\n",
        "            break;\n",
        "        default:\n",
        "            printf(\"Invalid kernel number\\n\");\n",
        "            exit(1);\n",
        "    }\n",
        "}\n",
        "\n",
        "// ----------------------------------------------------------------------------\n",
        "\n",
        "int main(int argc, char **argv) {\n",
        "    srand(0);\n",
        "\n",
        "    int B = 8;\n",
        "    int T = 1024;\n",
        "    int C = 768;\n",
        "\n",
        "    int deviceIdx = 0;\n",
        "    cudaCheck(cudaSetDevice(deviceIdx));\n",
        "\n",
        "    // create host memory of random numbers\n",
        "    float* out = (float*)malloc(B * T * C * sizeof(float));\n",
        "    float* rms = (float*)malloc(B * T * sizeof(float));\n",
        "    float* inp = make_random_float(B * T * C);\n",
        "    float* weight = make_random_float(C);\n",
        "    float* bias = make_random_float(C);\n",
        "\n",
        "    // move to GPU\n",
        "    float* d_out;\n",
        "    float* d_rms;\n",
        "    float* d_inp;\n",
        "    float* d_weight;\n",
        "    float* d_bias;\n",
        "    cudaCheck(cudaMalloc(&d_out, B * T * C * sizeof(float)));\n",
        "    cudaCheck(cudaMalloc(&d_rms, B * T * sizeof(float)));\n",
        "    cudaCheck(cudaMalloc(&d_inp, B * T * C * sizeof(float)));\n",
        "    cudaCheck(cudaMalloc(&d_weight, C * sizeof(float)));\n",
        "    cudaCheck(cudaMalloc(&d_bias, C * sizeof(float)));\n",
        "    cudaCheck(cudaMemcpy(d_inp, inp, B * T * C * sizeof(float), cudaMemcpyHostToDevice));\n",
        "    cudaCheck(cudaMemcpy(d_weight, weight, C * sizeof(float), cudaMemcpyHostToDevice));\n",
        "    cudaCheck(cudaMemcpy(d_bias, bias, C * sizeof(float), cudaMemcpyHostToDevice));\n",
        "\n",
        "    // read kernel_num from command line\n",
        "    int kernel_num = 3;\n",
        "    if (argc > 1) {\n",
        "        kernel_num = atoi(argv[1]);\n",
        "    }\n",
        "    printf(\"Using kernel %d\\n\", kernel_num);\n",
        "\n",
        "    int block_sizes[] = {32, 64, 128, 256, 512, 1024};\n",
        "    float* out_gpu = (float*)malloc(B * T * C * sizeof(float));\n",
        "    float* rms_gpu = (float*)malloc(B * T * sizeof(float));\n",
        "\n",
        "    rmsnorm_forward_cpu(out, rms, inp, weight, bias, B, T, C);\n",
        "\n",
        "    // check the correctness of the kernel at all block sizes\n",
        "    for (int j = 0; j < sizeof(block_sizes) / sizeof(int); j++) {\n",
        "        int block_size = block_sizes[j];\n",
        "        printf(\"Checking block size %d.\\n\", block_size);\n",
        "\n",
        "        rmsnorm_forward(kernel_num, d_out, d_rms, d_inp, d_weight, d_bias, B, T, C, block_size);\n",
        "\n",
        "        validate_result(d_out, out, \"out\", B * T * C, 1e-5f);\n",
        "        validate_result(d_rms, rms, \"rms\", B * T, 1e-5f);\n",
        "    }\n",
        "\n",
        "    printf(\"All results match. Starting benchmarks.\\n\\n\");\n",
        "\n",
        "    // time the kernel at different block sizes\n",
        "    for (int j = 0; j < sizeof(block_sizes) / sizeof(int); j++) {\n",
        "        int block_size = block_sizes[j];\n",
        "\n",
        "        int repeat_times = 2000;\n",
        "        float elapsed_time = benchmark_kernel(\n",
        "                                repeat_times,\n",
        "                                rmsnorm_forward,\n",
        "                                kernel_num,\n",
        "                                d_out,\n",
        "                                d_rms,\n",
        "                                d_inp,\n",
        "                                d_weight,\n",
        "                                d_bias,\n",
        "                                B,\n",
        "                                T,\n",
        "                                C,\n",
        "                                block_size\n",
        "                            );\n",
        "\n",
        "        // napkin math: estimate the memory bandwidth achieved\n",
        "        // e.g. A100 40GB PCIe is advertised at 1,555GB/s\n",
        "        long memory_ops = (2 * B * T * C) * 4; // *4 for float\n",
        "        float memory_bandwidth = memory_ops / elapsed_time / 1e6;\n",
        "\n",
        "        printf(\"block_size %4d | time %.4f ms | bandwidth %.2f GB/s\\n\", block_size, elapsed_time, memory_bandwidth);\n",
        "    }\n",
        "\n",
        "    // free memory\n",
        "    free(out);\n",
        "    free(rms);\n",
        "    free(inp);\n",
        "    free(weight);\n",
        "    free(bias);\n",
        "    cudaCheck(cudaFree(d_out));\n",
        "    cudaCheck(cudaFree(d_rms));\n",
        "    cudaCheck(cudaFree(d_inp));\n",
        "    cudaCheck(cudaFree(d_weight));\n",
        "    cudaCheck(cudaFree(d_bias));\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -I /usr/local/cuda/samples/common/inc/ -L/usr/local/cuda/include -lcublas -lcusolver -O3 --use_fast_math -lcublas -std=c++17 rmsnorm_forward.cu -o rmsnorm_forward"
      ],
      "metadata": {
        "id": "yDWeYeu9SGTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./rmsnorm_forward"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4UT7oSSNRfey",
        "outputId": "be2e196b-963b-437a-e035-df366d071457"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using kernel 3\n",
            "Checking block size 32.\n",
            "1.101622 1.101622\n",
            "0.951610 0.951610\n",
            "1.522446 1.522446\n",
            "-1.583111 -1.583111\n",
            "-0.604489 -0.604489\n",
            "1.769185 1.769185\n",
            "1.743165 1.743165\n",
            "1.696036 1.696035\n",
            "1.725868 1.725868\n",
            "1.705505 1.705506\n",
            "Checking block size 64.\n",
            "1.101622 1.101622\n",
            "0.951610 0.951610\n",
            "1.522446 1.522446\n",
            "-1.583111 -1.583111\n",
            "-0.604489 -0.604489\n",
            "1.769185 1.769185\n",
            "1.743165 1.743165\n",
            "1.696036 1.696035\n",
            "1.725868 1.725868\n",
            "1.705505 1.705506\n",
            "Checking block size 128.\n",
            "1.101622 1.101622\n",
            "0.951610 0.951610\n",
            "1.522446 1.522446\n",
            "-1.583111 -1.583111\n",
            "-0.604489 -0.604489\n",
            "1.769185 1.769185\n",
            "1.743165 1.743165\n",
            "1.696036 1.696035\n",
            "1.725868 1.725868\n",
            "1.705505 1.705506\n",
            "Checking block size 256.\n",
            "1.101622 1.101622\n",
            "0.951610 0.951610\n",
            "1.522446 1.522446\n",
            "-1.583111 -1.583111\n",
            "-0.604489 -0.604489\n",
            "1.769185 1.769185\n",
            "1.743165 1.743165\n",
            "1.696036 1.696035\n",
            "1.725868 1.725868\n",
            "1.705505 1.705506\n",
            "Checking block size 512.\n",
            "1.101622 1.101622\n",
            "0.951610 0.951610\n",
            "1.522446 1.522446\n",
            "-1.583111 -1.583111\n",
            "-0.604489 -0.604489\n",
            "1.769185 1.769185\n",
            "1.743165 1.743165\n",
            "1.696036 1.696035\n",
            "1.725868 1.725868\n",
            "1.705505 1.705506\n",
            "Checking block size 1024.\n",
            "1.101622 1.101622\n",
            "0.951610 0.951610\n",
            "1.522446 1.522446\n",
            "-1.583111 -1.583111\n",
            "-0.604489 -0.604489\n",
            "1.769185 1.769185\n",
            "1.743165 1.743165\n",
            "1.696036 1.696035\n",
            "1.725868 1.725868\n",
            "1.705505 1.705506\n",
            "All results match. Starting benchmarks.\n",
            "\n",
            "block_size   32 | time 0.2429 ms | bandwidth 207.21 GB/s\n",
            "block_size   64 | time 0.2349 ms | bandwidth 214.25 GB/s\n",
            "block_size  128 | time 0.2248 ms | bandwidth 223.90 GB/s\n",
            "block_size  256 | time 0.2219 ms | bandwidth 226.82 GB/s\n",
            "block_size  512 | time 0.2198 ms | bandwidth 228.97 GB/s\n",
            "block_size 1024 | time 0.2591 ms | bandwidth 194.28 GB/s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile simple_test.c\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <math.h>\n",
        "#include <assert.h>\n",
        "\n",
        "// Root Mean Square Layernorm Forward Pass\n",
        "void rmsnorm_forward_cpu(\n",
        "    float *out,\n",
        "    float *rms,\n",
        "    const float *inp,\n",
        "    const float *weight,\n",
        "    const float *bias,\n",
        "    int B,\n",
        "    int T,\n",
        "    int C\n",
        ") {\n",
        "    const float eps = 1e-6f;\n",
        "\n",
        "    for (int b = 0; b < B; b++) {\n",
        "        for (int t = 0; t < T; t++) {\n",
        "            // seek to the input position inp[b,t,:]\n",
        "            const float* x = inp + b * T * C + t * C;\n",
        "            // compute RMS\n",
        "            float sum_of_squares = 0.0f;\n",
        "            for (int i = 0; i < C; i++) {\n",
        "                sum_of_squares += x[i] * x[i];\n",
        "            }\n",
        "            float rms_val = 1.0f / sqrtf(sum_of_squares / C + eps);\n",
        "            // seek to the output position in out[b,t,:]\n",
        "            float* out_bt = out + b * T * C + t * C;\n",
        "            for (int i = 0; i < C; i++) {\n",
        "                float n = x[i] * rms_val; // normalized output\n",
        "                float o = n * weight[i] + bias[i]; // scale and shift it\n",
        "                out_bt[i] = o; // write\n",
        "            }\n",
        "            // cache the rms for the backward pass later\n",
        "            rms[b * T + t] = rms_val;\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "// Input data\n",
        "float inp[2 * 3 * 4] = {\n",
        "    1.0, 2.0, 3.0, 4.0,\n",
        "    5.0, 6.0, 7.0, 8.0,\n",
        "    9.0, 10.0, 11.0, 12.0,\n",
        "    13.0, 14.0, 15.0, 16.0,\n",
        "    17.0, 18.0, 19.0, 20.0,\n",
        "    21.0, 22.0, 23.0, 24.0\n",
        "};\n",
        "\n",
        "// Weights and bias\n",
        "float weight[4] = {1.0, 1.0, 1.0, 1.0};\n",
        "float bias[4] = {0.0, 0.0, 0.0, 0.0};\n",
        "\n",
        "// Simplified main function to test a small batch\n",
        "int main() {\n",
        "    int B = 2;\n",
        "    int T = 3;\n",
        "    int C = 4;\n",
        "\n",
        "    // Outputs\n",
        "    float out[B * T * C];\n",
        "    float rms[B * T];\n",
        "\n",
        "    rmsnorm_forward_cpu(out, rms, inp, weight, bias, B, T, C);\n",
        "\n",
        "    // Print outputs for comparison\n",
        "    printf(\"Output:\\n\");\n",
        "    for (int i = 0; i < B * T * C; i++) {\n",
        "        printf(\"%f \", out[i]);\n",
        "        if ((i + 1) % C == 0) {\n",
        "            printf(\"\\n\");\n",
        "        }\n",
        "    }\n",
        "\n",
        "    printf(\"RMS:\\n\");\n",
        "    for (int i = 0; i < B * T; i++) {\n",
        "        printf(\"%f \", rms[i]);\n",
        "        if ((i + 1) % T == 0) {\n",
        "            printf(\"\\n\");\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZZN9B_lNRr7",
        "outputId": "22b86c2c-9f29-4b5b-afc1-1e37ac322973"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting simple_test.c\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gcc simple_test.c -o simple_test -lm"
      ],
      "metadata": {
        "id": "_LHWjHbLNZgY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./simple_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nTqPoh4XOe-G",
        "outputId": "1332992d-9783-4546-da7c-3688f5f4aa47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "0.365148 0.730297 1.095445 1.460593 \n",
            "0.758098 0.909718 1.061337 1.212957 \n",
            "0.852325 0.947028 1.041730 1.136433 \n",
            "0.893898 0.962660 1.031421 1.100183 \n",
            "0.917245 0.971201 1.025157 1.079112 \n",
            "0.932183 0.976573 1.020963 1.065352 \n",
            "RMS:\n",
            "0.365148 0.151620 0.094703 \n",
            "0.068761 0.053956 0.044390 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, dim: int, eps: float = 1e-6):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.weight = nn.Parameter(torch.ones(dim))\n",
        "\n",
        "    def _norm(self, x):\n",
        "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self._norm(x.float()).type_as(x)\n",
        "        return output * self.weight\n",
        "\n",
        "# Input data\n",
        "B, T, C = 2, 3, 4\n",
        "inputs = torch.tensor([\n",
        "    [1.0, 2.0, 3.0, 4.0],\n",
        "    [5.0, 6.0, 7.0, 8.0],\n",
        "    [9.0, 10.0, 11.0, 12.0],\n",
        "    [13.0, 14.0, 15.0, 16.0],\n",
        "    [17.0, 18.0, 19.0, 20.0],\n",
        "    [21.0, 22.0, 23.0, 24.0]\n",
        "]).reshape(B, T, C)\n",
        "\n",
        "# PyTorch RMSNorm model\n",
        "model = RMSNorm(C)\n",
        "with torch.no_grad():\n",
        "    torch_output = model(inputs).numpy()\n",
        "\n",
        "print(\"PyTorch Output:\")\n",
        "print(torch_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8S3WV4k6N0S6",
        "outputId": "864228cf-7d4f-43a5-acba-297b3ddf966b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch Output:\n",
            "[[[0.36514837 0.73029673 1.0954452  1.4605935 ]\n",
            "  [0.75809807 0.9097177  1.0613372  1.2129569 ]\n",
            "  [0.8523247  0.9470275  1.0417303  1.136433  ]]\n",
            "\n",
            " [[0.8938984  0.96265984 1.0314212  1.1001827 ]\n",
            "  [0.91724545 0.97120106 1.0251567  1.0791123 ]\n",
            "  [0.9321832  0.9765729  1.0209626  1.0653522 ]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile llama3_rope_forward.c\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <math.h>\n",
        "\n",
        "// Reshape function to split the real and imaginary parts\n",
        "void reshape_complex(float* inp, int B, int T, int C, float* out_r, float* out_i) {\n",
        "    for (int b = 0; b < B; b++) {\n",
        "        for (int t = 0; t < T; t++) {\n",
        "            for (int c = 0; c < C/2; c++) {\n",
        "                int idx = b * T * C + t * C + 2 * c;\n",
        "                out_r[b * T * (C/2) + t * (C/2) + c] = inp[idx];\n",
        "                out_i[b * T * (C/2) + t * (C/2) + c] = inp[idx + 1];\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "void apply_rotary_emb_cpu(\n",
        "    float* xq_inp,\n",
        "    float* xk_inp,\n",
        "    float* freqs_cos,\n",
        "    float* freqs_sin,\n",
        "    float* xq_out,\n",
        "    float* xk_out,\n",
        "    int B,\n",
        "    int T,\n",
        "    int C\n",
        ") {\n",
        "    float* xq_r = (float*)malloc(B * T * (C/2) * sizeof(float));\n",
        "    float* xq_i = (float*)malloc(B * T * (C/2) * sizeof(float));\n",
        "    float* xk_r = (float*)malloc(B * T * (C/2) * sizeof(float));\n",
        "    float* xk_i = (float*)malloc(B * T * (C/2) * sizeof(float));\n",
        "\n",
        "    reshape_complex(xq_inp, B, T, C, xq_r, xq_i);\n",
        "    reshape_complex(xk_inp, B, T, C, xk_r, xk_i);\n",
        "\n",
        "    for (int b = 0; b < B; b++) {\n",
        "        for (int t = 0; t < T; t++) {\n",
        "            for (int c = 0; c < C/2; c++) {\n",
        "                int idx = b * T * (C/2) + t * (C/2) + c;\n",
        "                float xq_r_val = xq_r[idx];\n",
        "                float xq_i_val = xq_i[idx];\n",
        "                float xk_r_val = xk_r[idx];\n",
        "                float xk_i_val = xk_i[idx];\n",
        "\n",
        "                float cos_val = freqs_cos[c];\n",
        "                float sin_val = freqs_sin[c];\n",
        "\n",
        "                xq_out[idx * 2] = xq_r_val * cos_val - xq_i_val * sin_val;\n",
        "                xq_out[idx * 2 + 1] = xq_r_val * sin_val + xq_i_val * cos_val;\n",
        "\n",
        "                xk_out[idx * 2] = xk_r_val * cos_val - xk_i_val * sin_val;\n",
        "                xk_out[idx * 2 + 1] = xk_r_val * sin_val + xk_i_val * cos_val;\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    free(xq_r);\n",
        "    free(xq_i);\n",
        "    free(xk_r);\n",
        "    free(xk_i);\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int B = 2;\n",
        "    int T = 3;\n",
        "    int C = 4;\n",
        "\n",
        "    float* xq_inp = (float*)malloc(B * T * C * sizeof(float));\n",
        "    float* xk_inp = (float*)malloc(B * T * C * sizeof(float));\n",
        "    float* freqs_cos = (float*)malloc((C/2) * sizeof(float));\n",
        "    float* freqs_sin = (float*)malloc((C/2) * sizeof(float));\n",
        "    float* xq_out = (float*)malloc(B * T * C * sizeof(float));\n",
        "    float* xk_out = (float*)malloc(B * T * C * sizeof(float));\n",
        "\n",
        "    for (int i = 0; i < B * T * C; i++) {\n",
        "        xq_inp[i] = i + 1;\n",
        "        xk_inp[i] = i + 1;\n",
        "    }\n",
        "    for (int i = 0; i < C/2; i++) {\n",
        "        freqs_cos[i] = cos(i);\n",
        "        freqs_sin[i] = sin(i);\n",
        "    }\n",
        "\n",
        "    apply_rotary_emb_cpu(xq_inp, xk_inp, freqs_cos, freqs_sin, xq_out, xk_out, B, T, C);\n",
        "\n",
        "    for (int i = 0; i < B * T * C; i++) {\n",
        "        printf(\"xq_out[%d] = %f\\n\", i, xq_out[i]);\n",
        "        printf(\"xk_out[%d] = %f\\n\", i, xk_out[i]);\n",
        "    }\n",
        "\n",
        "    free(xq_inp);\n",
        "    free(xk_inp);\n",
        "    free(freqs_cos);\n",
        "    free(freqs_sin);\n",
        "    free(xq_out);\n",
        "    free(xk_out);\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SvarpOkvblQG",
        "outputId": "5dc54119-cb4d-45a3-8ab4-ef990a5942bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting llama3_rope_forward.c\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gcc llama3_rope_forward.c -o llama3_rope_forward -lm"
      ],
      "metadata": {
        "id": "6asojpNCbvaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./llama3_rope_forward"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txEALT3tb2AL",
        "outputId": "18e1035d-0db4-46b1-9dc6-32e2a4f5ff15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "xq_out[0] = 1.000000\n",
            "xk_out[0] = 1.000000\n",
            "xq_out[1] = 2.000000\n",
            "xk_out[1] = 2.000000\n",
            "xq_out[2] = -1.744977\n",
            "xk_out[2] = -1.744977\n",
            "xq_out[3] = 4.685622\n",
            "xk_out[3] = 4.685622\n",
            "xq_out[4] = 5.000000\n",
            "xk_out[4] = 5.000000\n",
            "xq_out[5] = 6.000000\n",
            "xk_out[5] = 6.000000\n",
            "xq_out[6] = -2.949652\n",
            "xk_out[6] = -2.949652\n",
            "xq_out[7] = 10.212715\n",
            "xk_out[7] = 10.212715\n",
            "xq_out[8] = 9.000000\n",
            "xk_out[8] = 9.000000\n",
            "xq_out[9] = 10.000000\n",
            "xk_out[9] = 10.000000\n",
            "xq_out[10] = -4.154326\n",
            "xk_out[10] = -4.154326\n",
            "xq_out[11] = 15.739808\n",
            "xk_out[11] = 15.739808\n",
            "xq_out[12] = 13.000000\n",
            "xk_out[12] = 13.000000\n",
            "xq_out[13] = 14.000000\n",
            "xk_out[13] = 14.000000\n",
            "xq_out[14] = -5.359001\n",
            "xk_out[14] = -5.359001\n",
            "xq_out[15] = 21.266901\n",
            "xk_out[15] = 21.266901\n",
            "xq_out[16] = 17.000000\n",
            "xk_out[16] = 17.000000\n",
            "xq_out[17] = 18.000000\n",
            "xk_out[17] = 18.000000\n",
            "xq_out[18] = -6.563675\n",
            "xk_out[18] = -6.563675\n",
            "xq_out[19] = 26.793995\n",
            "xk_out[19] = 26.793995\n",
            "xq_out[20] = 21.000000\n",
            "xk_out[20] = 21.000000\n",
            "xq_out[21] = 22.000000\n",
            "xk_out[21] = 22.000000\n",
            "xq_out[22] = -7.768351\n",
            "xk_out[22] = -7.768351\n",
            "xq_out[23] = 32.321087\n",
            "xk_out[23] = 32.321087\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "from typing import Tuple\n",
        "\n",
        "# Helper function to reshape for broadcasting\n",
        "def reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor):\n",
        "    ndim = x.ndim\n",
        "    assert 0 <= 1 < ndim\n",
        "    assert freqs_cis.shape == (x.shape[1], x.shape[-1])\n",
        "    shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]\n",
        "    return freqs_cis.view(shape)\n",
        "\n",
        "# Rotary Embedding Function\n",
        "def apply_rotary_emb(\n",
        "    xq: torch.Tensor,\n",
        "    xk: torch.Tensor,\n",
        "    freqs_cos: torch.Tensor,\n",
        "    freqs_sin: torch.Tensor\n",
        ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "\n",
        "    # reshape xq and xk to match the complex representation\n",
        "    xq_r, xq_i = xq.float().reshape(xq.shape[:-1] + (-1, 2)).unbind(-1)\n",
        "    xk_r, xk_i = xk.float().reshape(xk.shape[:-1] + (-1, 2)).unbind(-1)\n",
        "\n",
        "    # reshape freqs_cos and freqs_sin for broadcasting\n",
        "    freqs_cos = reshape_for_broadcast(freqs_cos, xq_r)\n",
        "    freqs_sin = reshape_for_broadcast(freqs_sin, xq_r)\n",
        "\n",
        "    # apply rotation using real numbers\n",
        "    xq_out_r = xq_r * freqs_cos - xq_i * freqs_sin\n",
        "    xq_out_i = xq_r * freqs_sin + xq_i * freqs_cos\n",
        "    xk_out_r = xk_r * freqs_cos - xk_i * freqs_sin\n",
        "    xk_out_i = xk_r * freqs_sin + xk_i * freqs_cos\n",
        "\n",
        "    # flatten last two dimensions\n",
        "    xq_out = torch.stack([xq_out_r, xq_out_i], dim=-1).flatten(3)\n",
        "    xk_out = torch.stack([xk_out_r, xk_out_i], dim=-1).flatten(3)\n",
        "\n",
        "    return xq_out.type_as(xq), xk_out.type_as(xk)\n",
        "\n",
        "# Input data\n",
        "B, T, C = 2, 3, 4\n",
        "xq_inp = torch.tensor([\n",
        "    [1.0, 2.0, 3.0, 4.0],\n",
        "    [5.0, 6.0, 7.0, 8.0],\n",
        "    [9.0, 10.0, 11.0, 12.0],\n",
        "    [13.0, 14.0, 15.0, 16.0],\n",
        "    [17.0, 18.0, 19.0, 20.0],\n",
        "    [21.0, 22.0, 23.0, 24.0]\n",
        "]).reshape(B, T, C)\n",
        "\n",
        "xk_inp = torch.tensor([\n",
        "    [1.0, 2.0, 3.0, 4.0],\n",
        "    [5.0, 6.0, 7.0, 8.0],\n",
        "    [9.0, 10.0, 11.0, 12.0],\n",
        "    [13.0, 14.0, 15.0, 16.0],\n",
        "    [17.0, 18.0, 19.0, 20.0],\n",
        "    [21.0, 22.0, 23.0, 24.0]\n",
        "]).reshape(B, T, C)\n",
        "\n",
        "# Precompute frequency components\n",
        "freqs_cos = torch.tensor([np.cos(i) for i in range(C//2)]).repeat(T, 1)\n",
        "freqs_sin = torch.tensor([np.sin(i) for i in range(C//2)]).repeat(T, 1)\n",
        "\n",
        "# Apply rotary embeddings using PyTorch\n",
        "xq_out, xk_out = apply_rotary_emb(xq_inp, xk_inp, freqs_cos, freqs_sin)\n",
        "\n",
        "print(\"xq_out:\")\n",
        "print(xq_out)\n",
        "print(\"xk_out:\")\n",
        "print(xk_out)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYlC-IYlcSGv",
        "outputId": "cf6c3da6-d6cd-49e2-8d49-2dcb5d159682"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "xq_out:\n",
            "tensor([[[[ 1.0000,  2.0000],\n",
            "          [-1.7450,  4.6856]],\n",
            "\n",
            "         [[ 5.0000,  6.0000],\n",
            "          [-2.9497, 10.2127]],\n",
            "\n",
            "         [[ 9.0000, 10.0000],\n",
            "          [-4.1543, 15.7398]]],\n",
            "\n",
            "\n",
            "        [[[13.0000, 14.0000],\n",
            "          [-5.3590, 21.2669]],\n",
            "\n",
            "         [[17.0000, 18.0000],\n",
            "          [-6.5637, 26.7940]],\n",
            "\n",
            "         [[21.0000, 22.0000],\n",
            "          [-7.7684, 32.3211]]]])\n",
            "xk_out:\n",
            "tensor([[[[ 1.0000,  2.0000],\n",
            "          [-1.7450,  4.6856]],\n",
            "\n",
            "         [[ 5.0000,  6.0000],\n",
            "          [-2.9497, 10.2127]],\n",
            "\n",
            "         [[ 9.0000, 10.0000],\n",
            "          [-4.1543, 15.7398]]],\n",
            "\n",
            "\n",
            "        [[[13.0000, 14.0000],\n",
            "          [-5.3590, 21.2669]],\n",
            "\n",
            "         [[17.0000, 18.0000],\n",
            "          [-6.5637, 26.7940]],\n",
            "\n",
            "         [[21.0000, 22.0000],\n",
            "          [-7.7684, 32.3211]]]])\n"
          ]
        }
      ]
    }
  ]
}